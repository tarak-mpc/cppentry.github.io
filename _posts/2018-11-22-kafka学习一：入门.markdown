---
layout:     post
title:      kafka学习一：入门
---
<div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">
								            <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f76675cdea.css">
						<div class="htmledit_views" id="content_views">
                
<h1>1.概述</h1>
<p>　　经过一个多月的时间观察，业务上在集成Kafka后，各方面还算稳定，这里打算抽时间给大家分享一下Kafka在实际场景中的一些使用心得。本篇博客打算先给大家入个门，让大家对Kafka有个初步的了解，知道Kafka是做什么的，下面是本篇博客的目录内容：</p>
<ul><li>Kafka背景</li><li>Kafka应用场景</li><li>Kafka架构原理</li></ul><p>　　下面开始今天的博客分享内容。</p>
<h1>2.Kafka背景</h1>
<p>　　Kafka它本质上是一个消息系统，由当时从LinkedIn出来创业的三人小组开发，他们开发出了Apache Kafka实时信息队列技术，该技术致力于为各行各业的公司提供实时数据处理服务解决方案。Kafka为LinkedIn的中枢神经系统，管理从各个应用程序的汇聚，这些数据经过处理后再被分发到其他地方。Kafka不同于传统的企业信息队列系统，它是以近乎实时的方式处理流经一个公司的所有数据，目前已经服务于LinkedIn、Netflix、Uber以及Verizon，并为此建立了实时信息处理平台。</p>
<p>　　流水数据是所有站点对其网站使用情况做报表时都要用到的数据中最常用的一部分，流水数据包括PV，浏览内容信息以及搜索记录等。这些数据通常是先以日志文件的形式存在，然后有周期的去对这些日志文件进行统计分析处理，然后获得需要的KPI指标结果。</p>
<h1>3.Kafka应用场景</h1>
<p>　　我们在接触一门新技术或是新语言时，得明白这门技术（或是语言）的应用场景，也就说要明白它能做什么，服务的对象是谁，下面用一个图来说明，如下图所示：</p>
<p><br></p>
<p><img src="https://img-blog.csdn.net/20160509152029309?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p>首先，Kafka可以应用于消息系统，比如，当下较为热门的消息推送，这些消息推送系统的消息源，可以使用Kafka作为系统的核心组建来完成消息的生产和消息的消费。然后是网站的行迹，我们可以将企业的Portal，用户的操作记录等信息发送到Kafka中，按照实际业务需求，可以进行实时监控，或者做离线处理等。最后，一个是日志收集，类似于Flume套件这样的日志收集系统，但Kafka的设计架构采用push/pull，适合异构集群，Kafka可以批量提交消息，对Producer来说，在性能方面基本上是无消耗的，而在Consumer端中，我们可以使用HDFS这类的分布式文件存储系统进行存储。</p>
<h1>4.Kafka架构原理</h1>
<p></p>
<p>　　Kafka的设计之初是希望做一个统一的信息收集平台，能够实时的收集反馈信息，并且具有良好的容错能力。Kafka中我们最直观的感受就是它的消费者与生产者，如下图所示：</p>
<h2>4.1Producer And Consumer</h2>
<p><img src="https://img-blog.csdn.net/20160509152105294?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p><br></p>
<p>这里Kafka对消息的保存是根据Topic进行归类的，由消息生产者（Producer）和消息消费者（Consumer）组成，另外，每一个Server称为一个Broker。对于Kafka集群而言，Producer和Consumer都依赖于ZooKeeper来保证数据的一致性。</p>
<h2>4.2Topic</h2>
<p></p>
<p>　　在每条消息输送到Kafka集群后，消息都会由一个Type，这个Type被称为一个Topic，不同的Topic的消息是分开存储的。如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20160509152144795?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p><br></p>
<p>一个Topic会被归类为一则消息，每个Topic可以被分割为多个Partition，在每条消息中，它在文件中的位置称为Offset，用于标记唯一一条消息。在Kafka中，消息被消费后，消息仍然会被保留一定时间后在删除，比如在配置信息中，文件信息保留7天，那么7天后，不管Kafka中的消息是否被消费，都会被删除；以此来释放磁盘空间，减少磁盘的IO消耗。</p>
<p>　　在Kafka中，一个Topic的多个分区，被分布在Kafka集群的多个Server上，每个Server负责分区中消息的读写操作。另外，Kafka还可以配置分区需要备份的个数，以便提高可用行。由于用到来ZK来协调，每个分区都有一个Server为Leader状态，服务对外响应（如读写操作），若该Leader宕机，会由其他的Follower来选举出新的Leader来保证集群的高可用性。</p>
<h1>5.总结</h1>
<p>　　总体来说，介绍Kafka的相关背景，概述及原理，这些较为偏理论，概念性较强，需要大家认真的去理解、琢磨，这里可以大致熟悉一下，心中有个轮廓，后面会陆续介绍Kafka的实战用法，让大家在实际业务和编码中去体会Kafka的这些原理。</p>
<h1>6.结束语</h1>
<p>　　这篇博客就和大家分享到这里，如果大家在研究学习的过程当中有什么问题，可以加群进行讨论或发送邮件给我，我会尽我所能为您解答，与君共勉！</p>
<br><p><br></p>
<p><br></p>
            </div>
                </div>